{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on data analysis\n",
    "\n",
    "# 动手学数据分析 吗v你美女可能你是v你\n",
    "\n",
    "## 项目初衷\n",
    "这件事始于datawhale以前的数据分析课程，那时我作为一名学员的以《python for data analysis》这本书为教材教材，通过刷这本教材的代码来学习数据分析，书里对于pandas和numpy操作讲的很细，但是对于数据分析的逻辑的内容，就少了很多。所以很多学习者和我学完之后发现，敲了一堆代码并不知道它们有什么用。然后我也上过datawhale的另一门课程—数据挖掘实战。这门课程又比较偏模型和实战，直接给你一个任务，让你去完成，上手难度比较大，但是它的实战性可以让你对于什么是数据挖掘，以及数据挖掘的逻辑有很好的把握。所以有没有这样一门课，以项目为主线，将知识点孕育其中，通过边学，边做以及边被引导的方式来使学习效果达到更好，学完之后既能掌握pandas等的知识点又能掌握数据分析的大致思路和流程。通过调查发现，市面上这样的项目好像没有可以完全符合这样的标准（失望.jpg）。所以datawhale的小伙伴一起来做一门这样的开源课程，完成上面所说的那些小目标，让所有使用了我们课程的小伙伴可以更好的开启他的数据分析之路。\n",
    "\n",
    "这门课程现在是1.0版本，从基础的数据分析操作和数据分析流程讲起。之后会不断加入新的内容（比如数据挖掘的算法之类的）。这是开源课程，会不断迭代，大家共同参与，一起努力。\n",
    "\n",
    "既然这是一门诞生于datawhale的课程，学习它的时候搭配datawhale所配备其他资源会更好。我们提供的代码是jupyter形式的，里面有你所要完成的任务，也有我们给你的提示和引导，所以这样的形式再结合datawhale的[组队学习](https://github.com/datawhalechina/team-learning)，可以和大家一起讨论，一起补充资料，那么学习效果一定会加倍。还有，datawhale之前开源了一门pandas的教程—[Joyful-Pandas](https://github.com/datawhalechina/joyful-pandas)。里面梳理了Pandas的逻辑以及代码展示，所以在我们数据分析的课程中，关于Pandas的操作，你可以参考*Joyful-Pandas*，可以让你的数据分析学习事半功倍。\n",
    "\n",
    "关于我们项目的名字——动手学数据分析（Hands-on data analysis）。数据分析是一个要从一堆数字中看到真相的过程。学会操作数据只是数据分析的一半功力，剩下的另一半要用我们的大脑，多多思考，多多总结，更要多动手，实打实的的敲代码。所以也希望在学习这门课时，多去推理，多去问问为什么；多多练习，确保理论实践结合起来，在课程结束的时候一定会有大收获。\n",
    "\n",
    "\n",
    "## 课程编排与服用方法\n",
    "课程现分为三个单元，大致可以分为：数据基础操作，数据清洗与重构，建模和评估。\n",
    "\n",
    "1. 第一部分：我们获得一个要分析的数据，我要学会如何加载数据，查看数据，然后学习Pandas的一些基础操作，最后开始尝试探索性的数据分析。\n",
    "2. 第二部分：当我们可以比较熟练的操作数据并认识这个数据之后，我们需要开始数据清洗以及重构，将原始数据变为一个可用好用的数据，为之后放入模型做准备\n",
    "3. 第三部分：我们根据任务需求不同，要考虑建立什么模型，我们接触流行的sklearn库，建立模型。然后一个模型的好坏，我们是需要评估的，之后我们会引入模型评估的一些改变和实现。\n",
    "\n",
    "#### 服用方法\n",
    "\n",
    "我们的代码都是jupyter形式，每个部分的课程都分为课程和答案两个部分。学习期间，在课程代码中，完成所有的学习，自己查找资料，自己完成里面的代码操作，思考部分以及心得。之后可以和小伙伴讨论，分享资料和心得。关于答案部分，大家可以参考，但是由于数据分析本身是开放的，所以答案也是开放式的，更多希望大家可以有自己理解和答案。\n",
    "\n",
    "\n",
    "## 反馈\n",
    "* 如果有任何想法可以联系邮箱（chenands@qq.com）\n",
    "* 欢迎大家提issues\n",
    "\n",
    "## 成员名单\n",
    "金娟娟，陈安东，杨佳达，老表，李玲，张文涛，高立业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写在最前面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这门课程得主要目的是通过真实的数据，以实战的方式了解数据分析的流程和熟悉数据分析python的基本操作。知道了课程的目的之后，我们接下来我们要正式的开始数据分析的实战教学，完成kaggle上[泰坦尼克的任务](https://www.kaggle.com/c/titanic/overview)，实战数据分析全流程。\n",
    "这里有两份资料需要大家准备：\n",
    "图书《Python for Data Analysis》第六章和 baidu.com &\n",
    "google.com（善用搜索引擎）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 第一章：数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 载入数据\n",
    "数据集下载 https://www.kaggle.com/c/titanic/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 任务一：导入numpy和pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T12:04:38.296350Z",
     "start_time": "2025-12-06T12:04:38.033918Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【提示】如果加载失败，学会如何在你的python环境下安装numpy和pandas这两个库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 任务二：载入数据\n",
    "(1) 使用相对路径载入数据  \n",
    "(2) 使用绝对路径载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T12:04:40.595355Z",
     "start_time": "2025-12-06T12:04:40.572353Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T12:53:39.123962Z",
     "start_time": "2025-12-06T12:53:39.113962Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"E:\\pycharm\\Python3_9\\hands-on-data-analysis\\第一单元项目集合\\train.csv\"\n",
    "# 注意：Windows系统的路径中反斜杠\\需要用双反斜杠\\\\或者斜杠/来表示，否则会报错\n",
    "\n",
    "df = pd.read_table('E:\\\\pycharm\\\\Python3_9\\\\hands-on-data-analysis\\\\第一单元项目集合\\\\train.csv',sep=',')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【提示】相对路径载入报错时，尝试使用os.getcwd()查看当前工作目录。\n",
    "- cwd 是 Current Working Directory 的缩写\n",
    "\n",
    "【思考1.】知道数据加载的方法后，试试pd.read_csv()和pd.read_table()的不同，如果想让他们效果一样，需要怎么做？了解一下'.tsv'和'.csv'的不同，如何加载这两个数据集？\n",
    "\n",
    "【总结】加载的数据是所有工作的第一步，我们的工作会接触到不同的数据格式（eg:.csv;.tsv;.xlsx）,但是加载的方法和思路都是一样的，在以后工作和做项目的过程中，遇到之前没有碰到的问题，要多多查资料吗，使用google，了解业务逻辑，明白输入和输出是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T12:49:43.599380Z",
     "start_time": "2025-12-06T12:49:43.591097Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 打印当前工作目录\n",
    "print(\"当前工作目录:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【思考1.】\n",
    " `pd.read_csv()` 和 `pd.read_table()` 的区别，以及如何处理 `.csv` 和 `.tsv` 文件。\n",
    "\n",
    "## 1\\. `pd.read_csv()` 与 `pd.read_table()` 的主要区别\n",
    "\n",
    "这两个函数在 Pandas 中都用于从文本文件加载数据到 DataFrame 中，但它们被设计用来处理**不同默认分隔符**的文件。\n",
    "\n",
    "| 特性 | `pd.read_csv()` | `pd.read_table()` |\n",
    "| :--- | :--- | :--- |\n",
    "| **默认分隔符** (`sep` 参数) | **逗号** (`,`) | **制表符** (`\\t`) |\n",
    "| **使用场景** | 主要用于 `.csv` 文件（Comma Separated Values）。 | 主要用于 `.tsv` 文件（Tab Separated Values）或**通用文本文件**。 |\n",
    "| **默认索引列** | **无**（`index_col=None`） | **无**（`index_col=None`） |\n",
    "\n",
    "在功能上，`pd.read_csv()` **是更常用的和推荐的**。实际上，`pd.read_table()` 只是 `pd.read_csv()` 的一个包装器（Wrapper），它在内部调用 `pd.read_csv()` 并自动将分隔符设置为制表符。\n",
    "\n",
    "### 📌 如何让它们效果一样？\n",
    "\n",
    "要让 `pd.read_csv()` 和 `pd.read_table()` 读取同一个文件并产生相同的效果，你需要手动设置它们的分隔符 (`sep`) 参数，使其保持一致：\n",
    "\n",
    "#### 场景 A：读取逗号分隔的文件（`.csv`）\n",
    "\n",
    "| 函数 | 代码 | 说明 |\n",
    "| :--- | :--- | :--- |\n",
    "| `pd.read_csv()` | `pd.read_csv('data.csv')` | **使用默认值**：逗号 |\n",
    "| `pd.read_table()` | `pd.read_table('data.csv', sep=',')` | **手动指定**分隔符为逗号 |\n",
    "**sep 是 Separator（分隔符）的缩写**\n",
    "\n",
    "\n",
    "#### 场景 B：读取制表符分隔的文件（`.tsv`）\n",
    "\n",
    "| 函数 | 代码 | 说明 |\n",
    "| :--- | :--- | :--- |\n",
    "| `pd.read_csv()` | `pd.read_csv('data.tsv', sep='\\t')` | **手动指定**分隔符为制表符 |\n",
    "| `pd.read_table()` | `pd.read_table('data.tsv')` | **使用默认值**：制表符 |\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. 📄 `.tsv` 与 `.csv` 的区别及加载方法\n",
    "\n",
    "`.csv` 和 `.tsv` 都是常用的纯文本数据存储格式，它们的主要区别在于字段（列）之间是如何被分隔开的。\n",
    "\n",
    "### 什么是分隔符？\n",
    "\n",
    "**分隔符**（Delimiter）是一种特殊的字符，用于将数据行中的不同字段分开。\n",
    "\n",
    "| 文件后缀 | 全称 | 含义 | 分隔符 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **.csv** | **C**omma **S**eparated **V**alues | 逗号分隔值 | **逗号** (`,`) |\n",
    "| **.tsv** | **T**ab **S**eparated **V**alues | 制表符分隔值 | **制表符** (`\\t`) |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要读取 `.xlsx` 文件（Microsoft Excel 文件），需要使用 **Pandas** 配合一个专门处理 Excel 格式的**引擎**。\n",
    "\n",
    "用于读取 `.xlsx` 文件的 Pandas 函数是 `pd.read_excel()`。\n",
    "\n",
    "-----\n",
    "\n",
    "## 💻 读取 `.xlsx` 文件的步骤\n",
    "\n",
    "### 1\\. 安装必要的库\n",
    "\n",
    "Pandas 自己无法直接处理 `.xlsx` 这种复杂格式的 Excel 文件，它需要一个后端库作为“引擎”来解析文件。最常用的库是 **`openpyxl`**。\n",
    "\n",
    "如果你还没有安装，需要在命令行或终端运行以下命令：\n",
    "\n",
    "```bash\n",
    "pip install pandas openpyxl\n",
    "```\n",
    "\n",
    "### 2\\. 使用 `pd.read_excel()`\n",
    "\n",
    "安装完 `openpyxl` 后，你就可以在 Python 中使用 `pd.read_excel()` 函数来读取 `.xlsx` 文件了。\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 假设你的文件名为 'my_data.xlsx'\n",
    "file_path = 'my_data.xlsx'\n",
    "\n",
    "# 使用 pd.read_excel() 读取文件\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(df.head())\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 任务三：每1000行为一个数据模块，逐块读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T13:09:25.217211Z",
     "start_time": "2025-12-06T13:09:25.188205Z"
    }
   },
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('train.csv', chunksize=100)\n",
    "# chunksize=1000\t关键参数： 指定每次读取文件时，要读取的行数。这里设置为 1000 行。\n",
    "\n",
    "print(f\"chunker 的类型是: {type(chunker)}\")\n",
    "\n",
    "# 2. 遍历 chunker\n",
    "chunk_count = 0\n",
    "for chunk in chunker:\n",
    "    chunk_count += 1\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"这是第 {chunk_count} 个数据块 (chunk)\")\n",
    "    print(f\"数据块的类型是: {type(chunk)}\")\n",
    "    print(f\"数据块的行数是: {len(chunk)}\")\n",
    "    print(\"\\n数据块 (chunk) 的具体内容:\")\n",
    "\n",
    "    # 打印每个数据块的前几行，以展示它是 DataFrame\n",
    "    print(chunk.head(3))\n",
    "\n",
    "# 'chunker' 每次迭代都会返回一个包含 1000 行数据的 DataFrame\n",
    "for chunk in chunker:\n",
    "    # 这里的 'chunk' 就是一个标准的 Pandas DataFrame，但它只有 1000 行。\n",
    "\n",
    "    # 可以在这里对这 1000 行数据进行计算、筛选或聚合等操作\n",
    "    print(f\"当前数据块的行数: {len(chunk)}\")\n",
    "\n",
    "    # 示例：计算每块数据的平均值\n",
    "    average = chunk['Age'].mean()\n",
    "    print(f\"平均值: {average}\")\n",
    "\n",
    "    # 如果需要，可以将每块处理结果保存到外部列表或文件中\n",
    "    # results.append(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【思考1.】什么是逐块读取？为什么要逐块读取呢？\n",
    "\n",
    "【提示】大家可以chunker(数据块)是什么类型？用`for`循环打印出来出处具体的样子是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【思考1.】什么是逐块读取？为什么要逐块读取呢？\n",
    "\n",
    "Chunks是一个使用 **Pandas** 库来读取大型 CSV 文件时非常高效的技巧。\n",
    "\n",
    "核心作用是：**“不要一次性把整个大文件加载到内存中，而是把它分成一个个小的‘数据块’（Chunks），可以逐块处理。”**\n",
    "\n",
    "-----\n",
    "`chunker = pd.read_csv('train.csv', chunksize=100)`\n",
    "\n",
    "## 💻 代码逐段解释\n",
    "\n",
    "| 代码段 | 含义 |\n",
    "| :--- | :--- |\n",
    "| `pd.read_csv(...)` | 告知 Pandas 库，我们要读取一个 CSV 文件。 |\n",
    "| `'train.csv'` | 指定要读取的文件名（假设这是一个很大的训练数据集文件）。 |\n",
    "| **`chunksize=1000`** | **关键参数：** 指定每次读取文件时，要读取的行数。这里设置为 **1000 行**。 |\n",
    "| `chunker = ...` | 将读取的结果赋值给一个名为 `chunker` 的变量。 |\n",
    "\n",
    "-----\n",
    "\n",
    "## 💡 `chunksize` 参数的作用和原理\n",
    "\n",
    "当一个文件非常大（比如有几百万行，甚至几 GB）时，你的计算机内存可能不足以一次性存储整个文件，或者一次性加载会导致程序运行缓慢甚至崩溃。\n",
    "\n",
    "设置了 `chunksize` 后，`pd.read_csv()` 就不会返回一个完整的 **DataFrame**，而是返回一个特殊的**迭代器 (Iterator)** 对象，我们在这里命名为 `chunker`。\n",
    "\n",
    "### 1\\. `chunker` 是什么？\n",
    "\n",
    "`chunker` **不是**一个完整的 DataFrame，它是一个可以**被循环**的对象。\n",
    "\n",
    "你可以把它想象成一个\\*\\*“文件搬运工”\\*\\*。你告诉搬运工：“每次只给我搬 1000 行数据。”\n",
    "\n",
    "### 2\\. 如何使用 `chunker`？\n",
    "\n",
    "你需要使用 **`for` 循环**来从 `chunker` 中逐块地取出数据，进行处理\n",
    "\n",
    "### 3\\. 为什么使用分块读取？\n",
    "\n",
    "  * **节省内存 (Memory Efficiency)：** 你的程序永远只在内存中处理 1000 行数据，而不是整个文件。\n",
    "  * **处理超大文件 (Big Data)：** 即使文件大小超过了你电脑的可用内存，也可以完整地处理它。\n",
    "  * **进度跟踪 (Progress Tracking)：** 可以清晰地知道处理数据的进度。\n",
    "\n",
    "总而言之，`chunker = pd.read_csv('train.csv', chunksize=1000)` 的作用就是**开启了分批加载大文件的模式，以提高内存使用效率和处理速度**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 任务四：将表头改成中文，索引改为乘客ID [对于某些英文资料，我们可以通过翻译来更直观的熟悉我们的数据]\n",
    "PassengerId => 乘客ID  \n",
    "Survived    => 是否幸存   \n",
    "Pclass      => 乘客等级(1/2/3等舱位)  \n",
    "Name        => 乘客姓名  \n",
    "Sex         => 性别                 \n",
    "Age         => 年龄                 \n",
    "SibSp       => 堂兄弟/妹个数  \n",
    "Parch       => 父母与小孩个数  \n",
    "Ticket      => 船票信息             \n",
    "Fare        => 票价                \n",
    "Cabin       => 客舱                \n",
    "Embarked    => 登船港口             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将表头改成中文并将索引改为乘客ID，可以通过以下几种方式实现：\n",
    "\n",
    "### 方法 1：在读取数据时直接指定列名和索引\n",
    "使用 `pd.read_csv()` 的 `names` 参数指定中文列名，同时通过 `index_col` 参数设置索引为乘客ID。\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\n",
    "    'train.csv',\n",
    "    names=['乘客ID', '是否幸存', '乘客等级', '乘客姓名', '性别', '年龄', '堂兄弟/妹个数', '父母与小孩个数', '船票信息', '票价', '客舱', '登船港口'],\n",
    "    index_col='乘客ID',\n",
    "    header=0  # 跳过原始表头\n",
    ")\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 2：读取后修改列名和索引\n",
    "先读取数据，再通过 `columns` 属性和 `set_index()` 方法修改列名和索引。\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 修改列名\n",
    "df.columns = ['乘客ID', '是否幸存', '乘客等级', '乘客姓名', '性别', '年龄', '堂兄弟/妹个数', '父母与小孩个数', '船票信息', '票价', '客舱', '登船港口']\n",
    "\n",
    "# 设置索引\n",
    "df = df.set_index('乘客ID')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 3：使用字典映射替换列名\n",
    "通过字典映射替换列名，适合列名较多且需要动态修改的情况。\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 创建英文到中文的映射字典\n",
    "column_mapping = {\n",
    "    'PassengerId': '乘客ID',\n",
    "    'Survived': '是否幸存',\n",
    "    'Pclass': '乘客等级',\n",
    "    'Name': '乘客姓名',\n",
    "    'Sex': '性别',\n",
    "    'Age': '年龄',\n",
    "    'SibSp': '堂兄弟/妹个数',\n",
    "    'Parch': '父母与小孩个数',\n",
    "    'Ticket': '船票信息',\n",
    "    'Fare': '票价',\n",
    "    'Cabin': '客舱',\n",
    "    'Embarked': '登船港口'\n",
    "}\n",
    "\n",
    "# 替换列名\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# 设置索引\n",
    "df.set_index('乘客ID', inplace=True)\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 4：逐列修改列名\n",
    "如果只需要修改部分列名，可以通过 `rename()` 方法逐列修改。\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 修改部分列名\n",
    "df.rename(columns={\n",
    "    'PassengerId': '乘客ID',\n",
    "    'Survived': '是否幸存'\n",
    "}, inplace=True)\n",
    "\n",
    "# 设置索引\n",
    "df.set_index('乘客ID', inplace=True)\n",
    "df.head()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T13:10:24.883733Z",
     "start_time": "2025-12-06T13:10:24.859732Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', names=['乘客ID','是否幸存','仓位等级','姓名','性别','年龄','兄弟姐妹个数','父母子女个数','船票信息','票价','客舱','登船港口'],index_col='乘客ID',header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【思考】所谓将表头改为中文其中一个思路是：将英文列名表头替换成中文。还有其他的方法吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法三inplace=True\n",
    "\n",
    "在 Pandas 中，`inplace=True` 是一个非常常见的参数，它的含义是：**直接在原对象上进行修改，而不是返回一个新的对象。**\n",
    "\n",
    "简单来说，就是**“原地修改”**。\n",
    "\n",
    "### 详细对比\n",
    "\n",
    "#### 1. 默认情况 (`inplace=False`)\n",
    "如果你不写这个参数，或者设置为 `False`（这是默认值），Pandas 不会改变原本的 `df` 变量，而是把修改后的结果作为一个**新的 DataFrame** 返回给你。你需要用变量去接收它。\n",
    "\n",
    "```python\n",
    "# ❌ 原来的 df 不会变\n",
    "df.rename(columns=column_mapping) \n",
    "\n",
    "# ✅ 需要赋值给变量才能保存修改\n",
    "df_new = df.rename(columns=column_mapping) \n",
    "# 或者覆盖原变量\n",
    "df = df.rename(columns=column_mapping)\n",
    "```\n",
    "\n",
    "#### 2. 使用 `inplace=True`\n",
    "如果你设置为 `True`，Pandas 会直接修改内存中的 `df` 对象。这个操作**没有返回值**（返回 `None`），但 `df` 本身变了。\n",
    "\n",
    "```python\n",
    "# ✅ 原来的 df 直接被修改了\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# ❌ 千万不要这样写，因为 inplace=True 返回的是 None\n",
    "# df = df.rename(columns=column_mapping, inplace=True)  <-- 这样 df 会变成 None\n",
    "```\n",
    "\n",
    "### 总结\n",
    "*   **`inplace=False` (默认)**：**“给我一份改好的复印件，原件不要动。”**（安全，适合链式调用）\n",
    "*   **`inplace=True`**：**“直接在原件上改。”**（省去赋值步骤，代码看起来简洁一些）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 初步观察\n",
    "导入数据后，你可能要对数据的整体结构和样例进行概览，比如说，数据大小、有多少列，各列都是什么格式的，是否包含null等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 任务一：查看数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pandas DataFrame 的 **`.info()`** 方法的运行结果，它提供了关于这个数据集（DataFrame）的**简洁、概要的信息**。\n",
    "\n",
    "我们可以将这些信息分为几个关键部分来理解：\n",
    "\n",
    "## 1. 整体结构信息\n",
    "\n",
    "* `<class 'pandas.core.frame.DataFrame'>`：\n",
    "    * **含义：** 确认这个对象是一个 **Pandas DataFrame** 类型，这是 Pandas 中最常用的二维（表格型）数据结构。\n",
    "* `Int64Index: 891 entries, 1 to 891`：\n",
    "    * **含义：** 描述数据的**行信息**。\n",
    "        * **总行数 (Entries)：** 共有 **891** 条记录（或行）。\n",
    "        * **索引 (Index)：** 索引类型是 64 位整数 (`Int64Index`)，范围是从 **1 到 891**。这表明数据的索引是从 1 开始的，而不是默认的 0 开始。\n",
    "* `Data columns (total 11 columns)`：\n",
    "    * **含义：** 描述数据的**列信息**。\n",
    "        * **总列数：** 共有 **11** 列。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 列详细信息（数据质量检查的核心）\n",
    "\n",
    "这部分是 `.info()` 输出最重要的部分，它详细列出了每一列的数据情况。\n",
    "\n",
    "| 序号 | 列名 (Column) | 非空值计数 (Non-Null Count) | 数据类型 (Dtype) | 缺失值情况 |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| 0 | **是否幸存** | 891 non-null | `int64` | **无缺失值** (891/891) |\n",
    "| 1 | **仓位等级** | 891 non-null | `int64` | **无缺失值** (891/891) |\n",
    "| 2 | **姓名** | 891 non-null | `object` | **无缺失值** |\n",
    "| 3 | **性别** | 891 non-null | `object` | **无缺失值** |\n",
    "| 4 | **年龄** | **714 non-null** | `float64` | **有缺失值** (177 个) |\n",
    "| 5 | **兄弟姐妹个数** | 891 non-null | `int64` | **无缺失值** |\n",
    "| 9 | **客舱** | **204 non-null** | `object` | **严重缺失** (687 个) |\n",
    "| 10 | **登船港口** | **889 non-null** | `object` | **有缺失值** (2 个) |\n",
    "\n",
    "### 关键数据类型解释：\n",
    "\n",
    "* **`int64` (Integer 64-bit)：** 64 位整数。通常用于存储**整数**数据（如计数、ID、等级）。\n",
    "* **`float64` (Float 64-bit)：** 64 位浮点数。用于存储**小数**数据（如年龄、票价）。\n",
    "* **`object`：** 字符串（文本）类型。用于存储**文本**数据（如姓名、客舱号）。\n",
    "\n",
    "### 缺失值分析：\n",
    "\n",
    "通过比较总行数 (891) 和 Non-Null Count，我们可以迅速发现数据集中的 **缺失数据 (Missing Values)**：\n",
    "\n",
    "* **年龄：** 只有 714 个非空值，意味着有 $891 - 714 = \\mathbf{177}$ 个缺失值。\n",
    "* **客舱：** 只有 204 个非空值，意味着有 $891 - 204 = \\mathbf{687}$ 个缺失值（缺失率非常高）。\n",
    "* **登船港口：** 只有 889 个非空值，意味着有 $\\mathbf{2}$ 个缺失值。\n",
    "\n",
    "这些缺失值需要在后续的数据清洗和预处理阶段进行处理（比如填充或删除）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.  资源使用信息\n",
    "\n",
    "* `dtypes: float64(2), int64(4), object(5)`：\n",
    "    * **含义：** 总结了数据类型在整个 DataFrame 中的分布。\n",
    "        * 有 2 列是 `float64` 类型。\n",
    "        * 有 4 列是 `int64` 类型。\n",
    "        * 有 5 列是 `object`（字符串）类型。\n",
    "* `memory usage: 83.5+ KB`：\n",
    "    * **含义：** 估算这个 DataFrame 在内存中所占用的空间大约是 **83.5 千字节 (KB)**。这个数值可以帮助你评估处理更大文件时的内存需求。\n",
    "\n",
    "总而言之，`.info()` 是进行**初步数据探索**和**数据质量检查**时最重要的一步，它让你快速了解数据的规模、类型和完整性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个非常经典的 Python 初学者问题，触及了**方法调用（Call）**与**对象引用（Reference）**的核心区别。\n",
    "\n",
    "简单来说：**带括号是“做动作”，不带括号是“看说明书”。**\n",
    "\n",
    "以下是详细对比分析：\n",
    "\n",
    "### 1. `df.info()` —— 执行方法（做动作）\n",
    "\n",
    "*   **含义**：这是在**调用**（Call）这个函数。你告诉 Python：“现在立刻运行 `info` 这个功能。”\n",
    "*   **结果**：Python 会执行 `info` 内部的代码，计算并打印出 DataFrame 的详细摘要（行数、列数、非空值、内存占用等）。\n",
    "*   **你在数据分析中需要的**：正是这个。\n",
    "\n",
    "**代码示例：**\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "\n",
    "**输出结果（示例）：**\n",
    "```text\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 891 entries, 1 to 891\n",
    "Data columns (total 12 columns):\n",
    " #   Column    Non-Null Count  Dtype  \n",
    "---  ------    --------------  -----  \n",
    " 0   是否幸存      891 non-null    int64  \n",
    " ... (省略中间内容) ...\n",
    "dtypes: float64(2), int64(5), object(5)\n",
    "memory usage: 90.5+ KB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `df.info` —— 查看对象（看说明书）\n",
    "\n",
    "*   **含义**：这是在**引用**这个属性。你告诉 Python：“我想看看 `df` 对象里名为 `info` 的那个东西是什么。”\n",
    "*   **结果**：Python 不会运行代码，而是告诉你：“这是一个绑定在 DataFrame 上的方法对象。”\n",
    "*   **你在数据分析中需要的**：通常不需要，除非你在做高级编程（比如把这个函数赋值给另一个变量）。\n",
    "\n",
    "**代码示例：**\n",
    "```python\n",
    "df.info\n",
    "```\n",
    "\n",
    "**输出结果（示例）：**\n",
    "```text\n",
    "<bound method DataFrame.info of      是否幸存  仓位等级     姓名     性别     年龄  ...  父母子女个数            船票信息     票价    客舱  登船港口\n",
    "乘客ID                                      ...                                         \n",
    "1       0     3  Braund...  male   22.0  ...       0       A/5 21171   7.25   NaN     S\n",
    "2       1     1  Cuming... female  38.0  ...       0        PC 17599  71.28   C85     C\n",
    "...\n",
    "[891 rows x 11 columns]>\n",
    "```\n",
    "*(注意：它只是打印出了这个方法的描述信息，并没有统计数据的非空值或类型)*\n",
    "\n",
    "---\n",
    "\n",
    "### 总结对比表\n",
    "\n",
    "| 特性 | `df.info()` (带括号) | `df.info` (不带括号) |\n",
    "| :--- | :--- | :--- |\n",
    "| **本质** | **函数调用 (Function Call)** | **对象引用 (Object Reference)** |\n",
    "| **比喻** | **按下**遥控器上的“播放”键 | **手指着**遥控器上的“播放”键 |\n",
    "| **作用** | 执行代码，计算并显示数据摘要 | 查看这个方法本身在内存中的样子 |\n",
    "| **是否有返回值** | 有（通常打印到控制台或返回 None） | 有（返回方法对象本身） |\n",
    "| **常用场景** | **99.9% 的情况**（查看数据信息） | 极少（调试或高阶函数式编程） |\n",
    "\n",
    "### 结论\n",
    "在做数据分析查看数据概况时，**请务必加上括号 `()`**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 任务二：观察表格前10行的数据和后15行的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 任务三：判断数据是否为空，为空的地方返回True，其余地方返回False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().head()\n",
    "# 展示前 5 行数据的缺失值分布情况（True 表示缺失）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【总结】上面的操作都是数据分析中对于数据本身的观察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【思考】对于一个数据，还可以从哪些方面来观察？找找答案，这个将对下面的数据分析有很大的帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了你已经掌握的查看头尾行 (`head/tail`)、基本信息 (`info`) 和缺失值 (`isnull`) 之外，数据观察（Exploratory Data Analysis, EDA）通常还需要从以下几个核心维度进行深入：\n",
    "\n",
    "### 1. 描述性统计分析 (Descriptive Statistics)\n",
    "这是最直接了解数据数值分布的方法。\n",
    "*   **方法**：`df.describe()`\n",
    "*   **观察点**：\n",
    "    *   **均值 (mean)** vs **中位数 (50%)**：如果两者相差很大，说明数据存在偏斜（Skewness）或异常值。\n",
    "    *   **标准差 (std)**：数据波动大不大？\n",
    "    *   **最小值 (min) / 最大值 (max)**：数据范围是否合理？（例如：年龄不应为负数，票价不应无穷大）。\n",
    "\n",
    "```python\n",
    "# 查看数值型列的统计摘要\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "### 2. 唯一值与计数 (Unique Values & Counts)\n",
    "对于分类数据（如性别、登船港口、客舱等级），我们需要知道有哪些类别以及每个类别的数量。\n",
    "*   **方法**：\n",
    "    *   `df['列名'].unique()`：查看有哪些唯一值。\n",
    "    *   `df['列名'].value_counts()`：查看每个类别有多少条数据。\n",
    "*   **观察点**：\n",
    "    *   是否存在拼写错误的类别（如 \"male\" 和 \"Male\"）？\n",
    "    *   样本是否均衡？（例如：幸存者和遇难者比例是否悬殊？）\n",
    "\n",
    "```python\n",
    "# 查看“性别”列有哪些值，以及各有多少人\n",
    "df['性别'].value_counts()\n",
    "```\n",
    "\n",
    "### 3. 数据类型与格式检查 (Data Types & Format)\n",
    "虽然 `info()` 给了大概的类型，但有时需要更细致的检查。\n",
    "*   **观察点**：\n",
    "    *   **数值是否被存成了字符串？**（例如票价列里混入了 \"$\" 符号导致变成 object 类型）。\n",
    "    *   **时间格式是否正确？**（例如日期是 \"2021-01-01\" 字符串还是 datetime 对象）。\n",
    "\n",
    "### 4. 异常值检测 (Outlier Detection)\n",
    "寻找那些“格格不入”的数据。\n",
    "*   **方法**：\n",
    "    *   **排序**：`df.sort_values(by='列名')`，看看最大和最小的几个值是否离谱。\n",
    "    *   **可视化（箱线图）**：虽然还没讲到绘图，但这是观察异常值最直观的方法。\n",
    "*   **观察点**：\n",
    "    *   比如泰坦尼克号数据中，是否有年龄超过 100 岁或票价高达 5000 的记录？\n",
    "\n",
    "### 5. 相关性分析 (Correlation)\n",
    "观察特征之间，或者特征与目标变量（是否幸存）之间的关系。\n",
    "*   **方法**：`df.corr()`\n",
    "*   **观察点**：\n",
    "    *   票价越高，幸存率越高吗？\n",
    "    *   年龄和兄弟姐妹数量有关联吗？\n",
    "\n",
    "### 总结代码示例\n",
    "\n",
    "你可以尝试运行以下代码来扩展你的观察视角：\n",
    "\n",
    "```python\n",
    "# 1. 统计摘要（快速看数值分布）\n",
    "print(\"--- 统计摘要 ---\")\n",
    "print(df.describe())\n",
    "\n",
    "# 2. 查看分类数据的分布（例如：查看男女比例）\n",
    "print(\"\\n--- 性别分布 ---\")\n",
    "print(df['性别'].value_counts())\n",
    "\n",
    "# 3. 排序观察（例如：看看票价最高的前 5 个人）\n",
    "print(\"\\n--- 票价最高的 5 人 ---\")\n",
    "print(df.sort_values(by='票价', ascending=False).head(5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个非常好的问题。理解 Pandas 如何定义“空值”对于数据清洗至关重要。\n",
    "\n",
    "### 1. `df.isnull().head()` 语句解释\n",
    "\n",
    "这行代码是两个方法的链式调用，我们可以把它拆解来看：\n",
    "\n",
    "1.  **`df.isnull()`**：\n",
    "    *   **作用**：它会遍历整个 DataFrame，判断每一个单元格是否为“空”。\n",
    "    *   **返回结果**：返回一个**与原 DataFrame 形状完全一样**的新 DataFrame，但里面的内容变成了布尔值（Boolean）：\n",
    "        *   如果原位置是空值，显示 `True`。\n",
    "        *   如果原位置有数据，显示 `False`。\n",
    "2.  **`.head()`**：\n",
    "    *   **作用**：取前 5 行。\n",
    "    *   **目的**：因为整个表可能很大，打印出来看不清，我们只看前 5 行的布尔值结果，快速检查一下哪些列可能存在缺失数据。\n",
    "\n",
    "**总结**：这行代码的作用是**“展示前 5 行数据的缺失值分布情况（True 表示缺失）”**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pandas 判断“空值”的机制\n",
    "\n",
    "Pandas 在判断是否为 `null` 时，主要依据底层 **NumPy** 的标准以及 Python 的对象标准。\n",
    "\n",
    "#### ✅ 被判定为 `True` (是空值) 的情况：\n",
    "\n",
    "Pandas 的 `isnull()` (以及它的别名 `isna()`) 会将以下情况视为缺失值：\n",
    "\n",
    "1.  **`NaN` (Not a Number)**：\n",
    "    *   这是标准的浮点数缺失值（IEEE 754 标准）。\n",
    "    *   这是 Pandas 中最常见的缺失值形式（数值列通常都用这个）。\n",
    "2.  **`None` (Python 的 None 对象)**：\n",
    "    *   Python 自带的“无”对象。\n",
    "    *   在 Pandas 的 `object` 类型（字符串/混合类型）列中，`None` 会被保留并视为缺失值。\n",
    "    *   在数值列中，Pandas 通常会自动把 `None` 转换为 `NaN`。\n",
    "3.  **`NaT` (Not a Time)**：\n",
    "    *   专门用于时间序列（Datetime）类型的缺失值。\n",
    "\n",
    "#### ❌ 被判定为 `False` (不是空值) 的情况（易错点）：\n",
    "\n",
    "很多初学者容易混淆以下情况，Pandas **不认为**它们是空值：\n",
    "\n",
    "1.  **空字符串 `\"\"` 或 `\" \"`**：\n",
    "    *   **注意**：这是最容易踩坑的地方！在 Pandas 看来，空字符串是一个长度为 0 的有效字符串对象，**不是 Null**。\n",
    "2.  **字符串 `\"NaN\"` 或 `\"null\"`**：\n",
    "    *   如果你读取 CSV 时没有正确解析，这些只是普通的文本字符，不是真正的缺失值。\n",
    "3.  **数字 `0`**：\n",
    "    *   0 是一个有效的数字，不是空。\n",
    "4.  **`False`**：\n",
    "    *   布尔值的 False 是有效值。\n",
    "5.  **`inf` (无穷大)**：\n",
    "    *   默认情况下，无穷大被视为有效值（可以通过配置修改）。\n",
    "\n",
    "### 代码演示\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    '数值': [1, np.nan, 0],             # np.nan 是空，0 不是\n",
    "    '对象': ['abc', None, ''],          # None 是空，空字符串 '' 不是\n",
    "    '时间': [pd.Timestamp('20210101'), pd.NaT, pd.Timestamp('20210102')] # NaT 是空\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "print(\"原始数据：\")\n",
    "print(df_test)\n",
    "print(\"-\" * 20)\n",
    "print(\"isnull() 判断结果：\")\n",
    "print(df_test.isnull())\n",
    "```\n",
    "\n",
    "**输出结果预演：**\n",
    "*   `np.nan` -> True\n",
    "*   `None` -> True\n",
    "*   `pd.NaT` -> True\n",
    "*   `0` -> **False**\n",
    "*   `''` (空字符串) -> **False**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 保存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 任务一：将你加载并做出改变的数据，在工作目录下保存为一个新文件train_chinese.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：不同的操作系统保存下来可能会有乱码。大家可以加入`encoding='GBK' 或者 ’encoding = ’utf-8‘‘`\n",
    "df.to_csv('train_chinese.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【总结】数据的加载以及入门，接下来就要接触数据本身的运算，我们将主要掌握numpy和pandas在工作和项目场景的运用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "582px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
